{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting...')\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lzma\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print('started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(use_cuda)\n",
    "n_neighbors = 10\n",
    "\n",
    "min_cluster_size = 100\n",
    "\n",
    "min_samples = 100\n",
    "\n",
    "\n",
    "umap_model = UMAP(n_neighbors = n_neighbors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the path to the compressed pickle file\n",
    "file_path = \"bills.pkl.xz\"\n",
    "\n",
    "# Open the compressed file in binary mode\n",
    "with lzma.open(file_path, \"rb\") as f:\n",
    "    # Load the pickle file using pandas\n",
    "    df = pd.read_pickle(f)\n",
    "\n",
    "print('done reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = df.sample(20000, random_state = 42)\n",
    "print(len(df), flush = True)\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Fit the vectorizer to the text data\n",
    "docs = df['Description'].astype(str).values\n",
    "\n",
    "topic_model = BERTopic.load(\"legal_model\")\n",
    "\n",
    "\n",
    "# assign cluster to bills\n",
    "\n",
    "a = topic_model.transform(df['Description'].astype(str).values)\n",
    "df['topic'] = a[0]\n",
    "topic_model.get_topic_info().to_csv(\"legiscan_info.csv\")\n",
    "\n",
    "df.to_pickle(\"bills3.pkl.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "This is the code to create the cluster, but since the model is already created, we don't need to re-create it. Also, it is non-deterministic\n",
    "so recreating the same model is unlikely to create the exact same groups\n",
    "\n",
    "'''\n",
    "topic_model.get_topic_info().to_csv(\"legiscan.csv\")\n",
    "umap_model = UMAP(n_neighbors = n_neighbors)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size = min_cluster_size, min_samples = min_samples, prediction_data=True)\n",
    "topic_model = BERTopic(umap_model = umap_model, hdbscan_model = hdbscan_model, \n",
    "                      top_n_words = 3,\n",
    "                      embedding_model = 'all-mpnet-base-v2',\n",
    "                      calculate_probabilities = False, verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Most of these visualizations are bad because we have 300+ groups. I think the visualize topics is interesting because it is interactive.\n",
    "The bar graph is also nice because it only shows a subset of topics.\n",
    "\n",
    "'''\n",
    "import plotly.io as pio\n",
    "# topics, probs = topic_model.fit_transform(docs)\n",
    "print('start plotting', flush = True)\n",
    "plot = topic_model.visualize_topics()\n",
    "\n",
    "pio.write_image(plot, \"topics.pdf\")\n",
    "print('next')\n",
    "plot3 = topic_model.visualize_barchart()\n",
    "pio.write_image(plot3, \"bar.png\")\n",
    "\n",
    "plot4 = topic_model.visualize_heatmap()\n",
    "pio.write_image(plot4, \"heat.png\")\n",
    "\n",
    "plot2 = topic_model.visualize_documents(docs)\n",
    "\n",
    "pio.write_image(plot2, \"docs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentiment analysis part\n",
    "\n",
    "much of the code is taken from the example \n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "'''\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "topic = []\n",
    "sent = []\n",
    "\n",
    "\n",
    "print(df['sentiment'].head())\n",
    "print(df['topic'].head())\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = df.sample(20000, random_state = 42) # file too large to do sentiment analysis on full dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    text = df['Description'].iloc[i]\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length = 512)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    sent.append(int(((np.argmax(scores)-1 *1))))\n",
    "\n",
    "#df['topic'] = a[0]\n",
    "df['sentiment'] = sent\n",
    "\n",
    "df.to_pickle(\"bills2.pkl.xz\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
